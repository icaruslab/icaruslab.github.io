[{"body":" Note Please read carefully, and don’t hesitate to send any questions you have before proceeding.  Who is this project for? Icarus Project and all its affiliated documentations, tools, and code are created and developed for the sole purpose of supporting Human rights organizations, independent new outlets, and individual publishers and activists struggling with Internet censorship in their environment.\nWhy not just use user-side tools, such as Tor or VPN?  While we are willing to document and facilitate the usability of different user-side circumvention solutions such as Tor, and \u0026gt;various VPN protocols, our main focus will be at publishers-side approaches.\n There are many reasons behind this approach\n User-side circumvention tools are often easy to be blocked or throttled.  With the exception of some circumvention tools with built-in obfuscation techniques such Shadowsocks, and new VPN protocols like Wireguard, most VPN services can be easy to identify and block. Beside the fact that implementing circumvention solutions in this scenario with protocols like OpenVPN such as SSL tunnelling can be very complicated and usually has its own consequences on the connection performance.\nAs for Tor, it will always be a very good and recommended solution, not just for censorship circumvention but also for user privacy related reasons, and enabling Tor bridges to overcome the network blockage is relatively easy and effective.\n Circumventing the block of user-side circumvention tools can be complicated process for non-technical users  As mentioned earlier, in a scenario where VPN services and/or Tor network is being effectively blocked or throttled, enabling and utilizing circumvention techniques for these services by the end user can be complicated especially for users with little or no technical experience.\n Publishers often lose big chunk of their non-technical audience  As a consequence of this situation, independent publishers in their struggle with Internet censorship often lose a considerable part of their audience due to the user’s inability to get around the censorship, in addition to that it also affects their ability to reach new audience.\n In some environments, installing circumvention tools i.e. Tor, VPN could be incriminating and dangerous  One big concern would be the user\u0026rsquo;s physical security issue as in some oppressive environments, physical abduction of the user’s devices is a possibility and the very existence of such software on these devices could be incriminating and often gets utilized by the authorities as evidence against users.\nCollateral freedom! In the context of Internet censorship circumvention, Collateral freedom concept can be described as increasing the economical, political, and technical costs of Internet censorship by using crucial and big cloud services providers, and utilizing modern web encryption protocols to ultimately force censors in a situation where they can\u0026rsquo;t block the content, without breaking or completely blocking access to crucial cloud services.\n Getting Started: Get started with Icarus Project  ","excerpt":"Note Please read carefully, and don’t hesitate to send any questions you have before proceeding. …","ref":"/docs/overview/","title":"Overview"},{"body":"Introduction and methodology What is the difference between Static and Dynamic websites Dynamic Websites can be identified as Websites written in scripting languages which need to be processed on the server before sending the final outcome (Web Pages) to the user/browser.\nSome examples of these languages would be PHP, Server-side Javascript, ASP etc. Typically, most of the modern day widely used Content management systems such as WordPress and Drupal, use PHP as a backend language, which means they need a fully operational server in order to work.\nA static website would be consisting of HTML and CSS and some client-side JavaScript variations, libraries, and frameworks, which could be served as it is to the browser which can process and render the required web pages from the code and display the final outcome to the user without the need for any server-side processing.\nWhy would you need to convert your website to a static one  The main advantage static websites holds is the ability to be stored on and served from virtually any kind of online storage platform.\n In these guides we will be going through the process of converting your dynamic website to a static one, or more accurately “creating a static mirror of your dynamic website”, the reason behind that will that we will be using some circumvention techniques which depends on and utilizes technologies which doesn’t the capability of server-side processing, therefore, it won’t be able to serve any web pages written in languages depends on this capability. One other main advantages would be security, speed, and overall performance. As static websites don\u0026rsquo;t have to run on servers and all code runs and renders locally at the user\u0026rsquo;s browser, that limits the attack surface significantly.\nWhat if you don’t want to abandon your CMS and fully migrate to a new stack It’s possible to keep your current setup as it is if you are using a CMS such as WordPress or Drupal. In which case we will go through the process of generating and maintaining a static mirror of your website. Throughout these guides, we will display different methods of achieving this task, with every method we will\nWhat is dynamic-mirroring  You can mirror a dynamic website in different ways without needing to generate a static clone of it.\n One of these ways will be creating a reverse-proxy server which acts as a middle box between the client and the origin server which hosts the website, the server in the middle will get the required URL from the client and contact the origin server to fetch the content and serve it to the user. In this method the reverse-proxy server will be reachable through a domain-name which isn’t blocked, this method is relatively easy to implement, but it’s easily blocked by blocking the domain name or the IP address of the proxy server.\nAnother method will be hosting a full copy of the original blocked website on the server and keeping it synchronized with the original server, this method needs certain access privileges to the origin server and it holds the same disadvantages of the first method. In many cases you can just keep changing the mirror server’s domain name and IP address whenever it gets blocked, but that can be inefficient in terms of both financial costs and usability.\nIn our guides we will be introducing a way of creating and maintaining a dynamic-mirror to any blocked website, the mirror will be hosted on Google Cloud Platform’s App Engine, and will be utilizing the ability to serve the website through a subdomain under *.appspot.com domain.  What is the level of technical expertise required in order to be able to implement the documented solutions Every published solution will be labeled according to the level of technical knowledge required for it\u0026rsquo;s implementation. Guides will be labeled in three different levels based on the required level of technical expertise you need to acquire/learn to be able to implement or develop the solution (Beginner - Intermediate - Advanced) \n","excerpt":"Introduction and methodology What is the difference between Static and Dynamic websites Dynamic …","ref":"/docs/getting-started/","title":"Getting Started"},{"body":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | Android | BSD\n This guide is an updated version of this article, published by MadaMasr - An Egyptian independent media organization - in the context of releasing their own Onion mirror.\n What is Tor?   Tor is the acronym for the original project that produced The Onion Router protocol, which protects the identity of internet users. It’s one of several technologies that have become widely used to roam the web freely and securely.  The project began in the mid-1990s and has gone through several phases. It is now administered by a not-for-profit organization, supervised by a community of its users and developers.\nAll Tor software is developed under open-source licenses and is publicly available for anyone to view and collaborate in improving.\nHow does Tor work? Let’s imagine the journey of a data packet between a user and a server (i.e. a website), both connected to the internet. With a regular connection, the data packet moves from the user’s device through the local router and then to the servers of the Internet Server Provider (ISP). If the ISP’s servers allow the packet to pass, it will reach its destination, hassle-free.\nBut what if you don’t want anyone to access your data packet, its destination or its content? Or to be able to block its journey?\nOnion routing allows your packet to take a different, more secure route to reach its final destination.\nYour Tor client will choose a group of random nodes for the packet to move through until it reaches its final destination. It will then generate a set of keys on your computer, which are used to encrypt the data packet as many times as the number of nodes it will pass through in its journey before reaching the final exit node.\n The encryption protocol allows each node the packet is passing through to decrypt just one layer of the encryption to get information about the next node the packet will pass through.\nThe aim of the onion routing protocol is for your packet to pass through a series of random nodes before reaching its final destination, with each node only receiving information about the nodes that directly proceed and follow it.\nNone of the nodes — except for the last one — can access information about the data packet’s final destination or see what’s inside it. The final exit node decrypts the final layer and directs the packet to its intended destination on the internet.\nWho runs Tor? The organization operates several machines that serve as nodes on its network. Volunteers run thousands more nodes — all under the community’s supervision — to ensure data security.  Anyone can volunteer to host a node in the network given that they have the necessary technical capabilities and internet speed.\nNeither the community nor the organization has the right or the ability to interfere with the encryption protocol because the encryption keys of each user are stored on their respective devices.\nData packets are therefore completely encrypted within the Tor network.  What’s the difference between using Tor and using a Virtual Private Network (VPN)?  VPN is the name given to different protocols and mechanisms that route the connection of one or more machines to a virtual network through which they can access the internet.\n To do so, the VPN client creates an encrypted connection to the server of the virtual private network, which acts like a tunnel transferring data from the machine to the server before it reaches the internet. When the data gets to its final destination, it appears as if it were sent from the server of the virtual network.\nVPN theoretically helps protect users’ anonymity as it hides their geographical locations and identities from the servers at the receiving end. Yet while VPN provides some privacy besides enabling users to access blocked websites, it doesn’t provide watertight protection.\nNetwork admins can collect users’ data after decrypting them. Data can also be manipulated if users connect to websites and services that don’t use secure protocols, such as HTTPS.\nIt’s also not advisable to use free VPN services, as many such services make money by collecting and selling user data. Others inadvertently transfer adware and, sometimes, malware from unprotected websites while browsing.\nThese are the main differences between using the Tor network and VPN services. Whether paid or free, VPN providers generally seek profit, and there isn’t a practical method to verify their user data security policies. Tor, on the other hand, is subject to clear transparency rules and the supervision of both developers and users.\nAdditionally, it’s easy to block access to VPN service providers, either through tracking their respective routing protocols or intercepting connections. Authorities can also directly block VPN servers and websites, as is the case with some countries.\nWhat’s the difference between Onion Services and the normal web - Clearnet? We’ve explained how Tor network operates by describing the journey of a data packet through the nodes until it reaches the exit node and then its destination on the internet.\nBut what if the packet never leaves Tor network?\nOnion services are used to grant access to servers operating completely within the Tor network that are only accessible using the onion routing protocol. These services can be accessed through randomly generated addresses (most of the time) ending with the special-use top-level domain suffix “.onion” (as opposed to the more common .com or .org).\nWhat this technique provides is a routing protocol between the user and the server so that neither has any information about the other.\nUnder optimum conditions in which all preferences are properly set by both parties, it is not possible to trace the server operating on Tor network, nor can the website’s admin collect any data that would reveal a user’s identity — barring, of course, a mistake or voluntary release of such information.  Tor Browser You can reach Tor Browser and download the installer for your respective operating system using the following links    Tor Project website If Internet censorship is in place in your country, there is a fairly chance this website is blocked\n  Tor Project repository on GitHub\n  The Internet Archive\n  Google Drive\n  Other mirrors\n  https://tor.eff.org/\n  https://tor.calyxinstitute.org/\n  https://tor.ccc.de/\n  We host an expermintal mirror for Tor Browser binary files on IPFS, you can reach it on ipfs.io/ipns/tor-ipfs.fightcensorship.tech\n  Tor Project also provides another method to download its browser. Just send an email to gettor@torproject.org including the name of your operating system (Windows, OSX, Linux). You will receive a message containing links to download the browser via Google Drive and Dropbox, which aren’t blocked.\n Tor Browser for Android is also available on the Google Play Store\nYou can also use Tor network’s proxy app Orbot, which provides a similar service to VPNs. Orbot redirects the data from all the apps on your phone to go through Tor network. This can help override blocks placed on apps such as Signal and Wire, among others.  There isn’t an official version of Tor Browser for iPhone and iPad users because of the restrictions Apple places on apps. However, every now and then, independent developers come up with alternative browsers to access Tor network such as OnionBrowser.\nThese browsers, however, don’t ensure the anonymity of users accessing content outside the Tor network.\n When Tor is blocked, activate bridges   Connection to Tor network usually gets restricted through blocking or jamming entry nodes because the addresses of most VPNs, as well as the protocols of nodes and relays, are openly available on the internet.  What makes fully blocking Tor network such a difficult task is that there are many ways to connect to it. One such way is by using bridges.\nIn a nutshell, bridges are servers run by volunteers whose addresses aren’t usually published openly. Bridges work as an intermediary between users and Tor network and help bypass blocks while hiding the connection from any party trying to analyze the network’s data.\nActivating Bridges Tor browser on Android phones\nTor’s Android client, Orbot\nTor browser on Desktop devices\nWebsites that can be accessed through .onion services   BBC\n  The New York Times\n  ProtonMail\n  Facebook\n  MadaMasr\n  ","excerpt":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | …","ref":"/docs/alternative-publishing-methods/tor/intro/","title":"Intro to Tor"},{"body":"Required expertise level : Advanced\nPlatform : Gnu/Linux | macOS | BSD\n Introduction  As described by it\u0026rsquo;s own developer\n EOTK provides a tool for deploying HTTP and HTTPS onion sites to provide official onion-networking presences for popular websites.\nThe result is essentially a \u0026ldquo;man in the middle\u0026rdquo; proxy; you should set them up only for your own sites, or for sites which do not require login credentials of any kind.\n Note A thorough implementation guide is available on the project\u0026rsquo;s repository on GitHub, therefore, we will be providing translations in other available languages for our guides.  ","excerpt":"Required expertise level : Advanced\nPlatform : Gnu/Linux | macOS | BSD\n Introduction  As described …","ref":"/docs/alternative-publishing-methods/tor/eotk/","title":"EOTK"},{"body":" In this guide we will be introducing two different methods of creating a mirror for your website on Tor network using Tor Onion services formerly known as - Tor Hidden Services.\nYou will also find a brief introduction to Tor network and how access it if it\u0026rsquo;s blocked in your country\n ","excerpt":" In this guide we will be introducing two different methods of creating a mirror for your website on …","ref":"/docs/alternative-publishing-methods/tor/","title":"Tor"},{"body":"Required expertise level : Intermediate / Advanced\nPlatform : Gnu/Linux | macOS | MS Windows | Android | BSD\n Decoy is a reverse-proxy written in python and ready to deploy on Google App Engine platform. It can be configured to serve a specific website through App Engine.\n The main advantage Decoy provides is the ability to serve the configured website through GAE platform - Google App Engine, using a sub-domain under *.appspot.com.\nDecoy will proxy all URLs which contains the configured domain in config.py file automatically, any other URL will be opened normally without proxy.\nUsage   Clone the project\ngit clone https://github.com/icaruslab/decoy.git\n  Create an account on Google Cloud Platform\n  After accessing Google Cloud Console create a new project\n  New Project\nNote The project name should be unique, if that\u0026rsquo;s the case the project ID will be the same, other wise it will be assigned a random name. The project ID will be used to access your mirror, final URL will be [ProjectID.appspot.com]  Install Google Cloud SDK Replace the url variable in config.py with your website domain name Use your terminal to go the project\u0026rsquo;s directory Run gcloud init to configure GCloud SDK and login to your account. Select the project you created earlier using gcloud config set project [ProjectID]  Set project ID\nRun gcloud app deploy and follow the instructions to deploy the app  Deploy\nFinally you can confirm your mirror is running by visiting [youprojectname].appspot.com  Why Google App Engine? The main advantage of having the mirror traffic routed through GAE, is the ability to use a sub-domain under appspot.com to access the mirror.\nAs the same domain is associated with many other Google cloud services it\u0026rsquo;d be difficult to block it entirely.\nHowever, it\u0026rsquo;s possible to block the sub-domain without having to block the entire domain name, in which case you\u0026rsquo;ll be able to get a new mirror up and running in less than two minutes by simply creating a new project with a different name and ID and repeating steps 8:10\nAttribution This project was inspired and built on mirrorrr project.\nDisclaimer This project and all Icarus Project’s related code and guides are developed for the sole purpose of Internet censorship circumvention, more specifically in human rights and independent media context.\nIcarus Project is not responsible for any abuse and/or malicious use of any of its published research results.\nTodo  Rewrite in Python3  ","excerpt":"Required expertise level : Intermediate / Advanced\nPlatform : Gnu/Linux | macOS | MS Windows | …","ref":"/docs/dynamic-mirroring/decoy/","title":"Decoy"},{"body":"Required expertise level : Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | Android | BSD\n Interplanetary File System - IPFS  is A peer-to-peer hypermedia protocol which is being introduced as the future successor of HTTP.\nBeing built on the peer-to-peer data exchange system, IPFS is practically difficult to block at least by current censorship techniques and standards, making it worth testing.\n While it\u0026rsquo;s basically built as a data storage and distribution protocol, makes the way it functions different than most current day web hosting and cloud functions today, where in most cases, websites are basically web applications which processes and renders on a server then send the resulted web pages to the user.\nIn IPFS case, there are no server nor processing capabilities, which mean that it\u0026rsquo;s only capable of serving static or semi-static web pages, essentially anything which depends on client-side rendering and processing. in this scenario, the browser gets to do most of job and IPFS just delivers raw files.\nTo learn about the difference between static and dynamic websites, refer the Getting Started section in the documentations.   Installing IPFS   MS Windows\n Install the CLI tool using Chocolatey windows package manager choco install ipfs Install the Desktop app (GUI) Chocolatey choco install ipfs-desktop Or download the installation file directly from IPFS website    Gnu/Linux\n Download and run the installation script for the Command Line Interface - CLI - tool from IPFS website Download and run the installation package for the Graphical User Interface - GUI - desktop app from GitHub    macOS\n Install using Homebrew package manager. brew install ipfs Download the Desktop app from GitHub    IPFS Desktop app\n Adding your static mirror to IPFS Adding your static mirror to IPFS is a very simple process but there are some configuration which should be considered to make sure your website will be easily accessible.   After following IPFS guides on how to get the application on your machine running, confirm  ❯ ipfs --version ipfs version 0.6.0  Using your termianl, go to your static mirror directory. Run ipfs add -Q --recursive --progress mystaticmirror   IPFS will start adding your files to it\u0026rsquo;s Datastore, and creating a unique hash for every file, the hash of the root directory will be the final output, this will be the address of your mirror on IPFS network.   Now, we can try to access our mirror using IPFS and the hash of the root directory.  There are two ways of accessing files on IPFS\n Using a gateway  IPFS Gateway - ipfs.io CloudFlare Gateway - cloudflare-ipfs.com Pinata Gateway - gateway.pinata.cloud    There are several online gateways that provides a bridge between normal web and data stored on IPFS, you can do that by going to the gateway in your browser while appending /ipfs/QmYn4A9jzkXob1iUPuwjpqxaMkrwE54ywvJUv2cE1s4egE to the URL.   using the local client (peer-to-peer)  This is the method IPFS should be functioning essentially.\nYou need to have IPFS installed on your machine for this method to work.\n  Install IPFS Companion browser extension and make sur IPFS is running on you machine. When you enable the extension, it detects if your local IPFS daemon is running in the background and automatically rewrites any IPFS URL that contains a valid hash to be fetched using your local IPFS installation.   Updating a website on IPFS Every time you update your static mirror with new content, you will need to add the new version to IPFS again.  As we mentioned earlier, IPFS depends on a hashing system to keep track of files, and since every file will create a unique hash, that means the final - root directory - hash will be different every time.\nThis situation presents a usability issue, as you will need somehow to share the new calculated hash with your audience every time you update your website, in order for them to be able to reach the most recent version of your website on IPFS.\nEnters the Interplanetary Name System (IPNS) IPNS provides a solution similar to the regular web Domain Name System (DNS).  Using a locally generated key on your machine, IPNS will create a static hash, which will be published to the Distributed hash table (DHT), this key can be linked to another hash and store it as value.\nThink about it (IPNS) as a domain name generator, it gives you a random hash, and register it in a table and store the most recent hash of your files as a value, IPFS users will just need the static IPNS hash to access your website.\n Generate a new IPFS key  ipfs key gen --type=rsa --size=2048 mystaticsite\nThe output will be your new IPNS address\n Publish your IPNS address and link it with your mirror\u0026rsquo;s most recent hash.  ipfs name publish --key=mystaticsite [replace with your files most recent hash]\nThe output will confirmation for the linking between the IPNS address and the IPFS hash\n We can confirm that your new IPNS address is alive either by accessing it using a gateway i.e. | ipfs.io/ipns/[put your IPNS hash here] |, or enable IPFS Companion browser extension to be redirected automatically to your local gateway  Note If you are adding large files/directories to IPFS.\nit might be a good idea to add --nocopy parameter in the first step:\nipfs add -Q --recursive --nocopy --progress mystaticmirror \nthis will tell IPFS not to make a full copy of every file in it\u0026rsquo;s datastore and will just create the hashes and use the original files, which will save disk space.\n ","excerpt":"Required expertise level : Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | Android | BSD …","ref":"/docs/alternative-publishing-methods/ipfs/","title":"Interplanetary File System"},{"body":"Convert a Dynamic site to a Static one As mentioned earlier, some suggested solutions in these guides are only capable of serving static web pages, in this section we’ll introduce some methods of generating a static clone of your dynamic website. Each of these suggested methods has their own pros and cons, we’ll try to iterate on each one of them as we tested them with different websites.\n","excerpt":"Convert a Dynamic site to a Static one As mentioned earlier, some suggested solutions in these …","ref":"/docs/static-mirroring/","title":"Static mirroring"},{"body":"Required expertise level : Advanced\nPlatform : Linux / Ubuntu - Debian\n This guide will walk you through the process of creating Onion Service for you static website.\n Note This guide focuses on creating a simple Onion service mainly in the context of censorship circumvention, if you are more concerned with the anonymity Tor provides in Onion services you should not depend on this guide alone, reading and understanding Tor project documentations will best practice in that case, as a ensuring full anonymity is an advanced and very details oriented process.    Install Nginx webserver   Run the following commands in your terminal in their respective order\n sudo apt install software-properties-common sudo add-apt-repository ppa:nginx/stable sudo apt update \u0026amp;\u0026amp; sudo apt install nginx -y Confirm your installation by entering nginx -v, the output should look similar to this\n Install Tor client  Note If Tor Project Website and Tor communications are blocked in your country, probably the official software repositories are also blocked, in which case you should skip adding them and install Tor directly from your distribution\u0026rsquo;s repositories.   Add in the following lines in /etc/apt/sources.list\n deb https://deb.torproject.org/torproject.org stretch main deb-src https://deb.torproject.org/torproject.org stretch main  Run the following commands in your terminal in their respective order\n curl https://deb.torproject.org/torproject.org/A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89.asc | gpg --import gpg --export A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89 | sudo apt-key add - sudo apt update \u0026amp;\u0026amp; apt install tor deb.torproject.org-keyring sudo apt update \u0026amp;\u0026amp; sudo apt install tor  Run the following commands to start Tor daemon\n sudo systemctl start tor sudo systemctl enable tor  Confirm Tor is running without issues\n sudo systemctl status tor tor --version  Configure Tor client   Open Tor config file at /etc/tor/torrc with your favorite editor\n vim /etc/tor/torrc\n Uncomment the following lines by removing the #, and optionally, replace the directory name in /var/lib/tor/hidden_service/ with different name, specially if you are planning on hosting multiple Onion Services on the same server. i.e. /var/lib/tor/myfirstonion/\n  Before:  72 #HiddenServiceDir /var/lib/tor/hidden_service/ 73 #HiddenServicePort 80 127.0.0.1:80  After:  HiddenServiceDir /var/lib/tor/myfirstonion/ HiddenServicePort 80 127.0.0.1:80  Restart Tor service\n sudo systemctl restart tor\n Confirm your Onion Service related files were generated at /var/lib/tor/myfirstonion/\n cd /var/lib/tor/myfirstonion/ \u0026amp;\u0026amp; ls You should find two files generated at this directory\n1- hostname contains your Onion Service address\n2- private_key Private key used for encryption. Don\u0026rsquo;t edit or share this file under any circumstances\n Configure Nginx Webserver  It\u0026rsquo;s very important to read Nginx documentations and follow the best practices when configuring your Onion Service\nBut essentially, you can get your Onion Service up \u0026amp; running by adding this simple config file to your /etc/nginx/sites-enabled\nserver { listen 127.0.0.1:80; server_name [onion-address]; #replace with your generated onion address, you can get that by executing : `cat /var/lib/[yourservicename]/hostname`  root /var/www/html/mystaticmirror; #replace with your mirror\u0026#39;s files directory, and make sure the webserver user has access permissions to it.  client_max_body_size 99M; port_in_redirect off; charset utf-8; index index.html; location / { autoindex off; } }  Testing your setup  Make sure Both Nginx and Tor client are restarted and running succesfully, then head to your Tor Browser and test your new Onion address.\n [Optional] - announcing your new mirror for Tor browser users  Onion-Location is a new feature implemented in Tor Browser.\nIt essentially allows you to announce your Onion mirror for Tor Browser users when they access your original website, through adding an HTTP Header using your webserver, or adding specific HTML \u0026lt;meta\u0026gt; to your index.html page.\n HTTP Header\n Apache  \u0026lt;VirtualHost *:443\u0026gt; ServerName \u0026lt;your-website.tld\u0026gt; DocumentRoot /path/to/htdocs Header set Onion-Location \u0026#34;http://your-onion-address.onion%{REQUEST_URI}s\u0026#34; SSLEngine on SSLCertificateFile \u0026#34;/path/to/www.example.com.cert\u0026#34; SSLCertificateKeyFile \u0026#34;/path/to/www.example.com.key\u0026#34; \u0026lt;/VirtualHost\u0026gt;  Nginx  add_header Onion-Location http://\u0026lt;your-onion-address\u0026gt;.onion$request_uri; HTML \u0026lt;meta\u0026gt;\n\u0026lt;meta http-equiv=\u0026#34;onion-location\u0026#34; content=\u0026#34;http://\u0026lt;your-onion-service-address\u0026gt;.onion\u0026#34; /\u0026gt;  When everything is well configured and working, visitors of your original Clearnet website using Tor browser, should be notified about the availability of the Onion mirror and it\u0026rsquo;s address.  ","excerpt":"Required expertise level : Advanced\nPlatform : Linux / Ubuntu - Debian\n This guide will walk you …","ref":"/docs/alternative-publishing-methods/tor/staticonion/","title":"Static Onion"},{"body":"Required expertise level : Advanced\nPlatform : Any\n What is Server Name Indication (SNI)? Server Name Indication (SNI) is an extension to the Transport Layer Security (TLS) network encrypted protocol.\nEssentially, SNI\u0026rsquo;s purpose is to send the requested domain name to the webserver in the form of plain/text HTTP header, so the server will send back the appropriate response to initiate the encrypted connection.  SNI was originally standardized in 2003, it\u0026rsquo;s wide implementation is a big part of today\u0026rsquo;s Internet infrastructure, mainly because it allowed hosting multiple websites with different domain names on the same webserver.\nSince then, the SNI header is a main target in any Deep Packet Inspection (DPI) operations, to preliminarily detect traffic to specific host before blocking. read more about SNI and the issues it presents\nWhat is Encrypted SNI and how can it help? Encrypted SNI (ESNI) is a new suggested extension, it uses symmetric encryption key to encrypt the SNI header, the server publishes the public key well-known DNS record and the client fetches the key using DNS.\nCurrent implementations of ESNI requires DNS over HTTPS (DoH) in order to work.\n Note Encrypted SNI is still not standardized, which means that it\u0026rsquo;s still in early development stages and might undergo a lot of changes on it\u0026rsquo;s way to be a widely implemented standard   How to test Encrypted SNI? Some projects are trying to test for the best implementation of ESNI, or how to integrate it with major software stacks i.e. Nginx, and OpenSSL.\nIf you are interested in a deeper technical understanding of ESNI you can check out these projects\n Developing ESNI for OpenSSL (DEfO) Noctilucent   In this guide, we will use the most stable and tested implementation which comes with CloudFlare and Mozilla Firefox  Server Side Currently, the only stable and tested server side implementation of ESNI is CloudFlare.\nAll you have to do is to enable TLS 1.3\nLog in to your CloudFlare account, select your domain name, go to \u0026gt; SSL/TLS options, choose \u0026gt; Edge Certificates, and make sure \u0026gt; TLS 1.3 is enabled.\nClient Side Mozilla Firefox latest releases are shipped with TLSv1.3 and ESNI extension support, yet, it doesn\u0026rsquo;t come enabled by default for now.\nTo enable ESNI support in Mozilla Firefox\n Make Firefox is updated to latest release   Enable DNS over HTTPS (DoH) in Preferences \u0026gt; Network Settings \u0026gt; Enable DNS over HTTPS  Note CloudFlare DoH resolver might be blocked in your country. Check here.\nIf that\u0026rsquo;s the case, you can try changing Next DNS, or choose Custom and test the public resolver - with DoH support - from this list.\n  Enable ESNI extension support  Open a new tab and type the address about:config Click Accept the Risk and Continue Search for network.security.esni.enabled Click on the value to change it from False to True   Restart your browser and test your settings here  Test your website by adding visiting this address in your browser https://[yourdomain.com]/cdn-cgi/trace i.e. https://www.cloudflare.com/cdn-cgi/trace\nYou should get these values:\ntls=TLSv1.3 sni=encrypted    Resources and readings  CloudFlare : Encrypt that SNI: Firefox edition CloudFlare : Encrypt it or lose it: how encrypted SNI works IETF : Transport Layer Security (TLS) Extensions IETF : TLS Encrypted Client Hello - draft-ietf-tls-esni-07 Chromium Support Status : Issue 908132 Developing ESNI for OpenSSL (DEfO) Mozilla : Encrypted SNI Comes to Firefox Nightly - October 18, 2018 If you want to test ESNI further, or develop something for it - Noctilucent For better understanding of DNS over HTTPS (DoH) and other DNS privacy related materials  ","excerpt":"Required expertise level : Advanced\nPlatform : Any\n What is Server Name Indication (SNI)? Server …","ref":"/docs/alternative-publishing-methods/esni/","title":"Encrypted SNI"},{"body":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | Android | BSD\n  Wget is a cli-based software that retrieves web content, it supports HTTP, HTTPS, and FTP protocols.  One of Wget’s features is the ability to scan and index the entirety of a website and download a fully functional static clone of the original website.\nThe static clone can be later refreshed and updated with new content published to the original website. While there are different ways of performing this task using Wget, you may get different results depending on your original website properties, including the CMS being used, the web server configurations, any kind of DDoS protection and online asset\u0026rsquo;s protection e.g. images and videos.\n Install Wget MS Windows\n  Chocolatey package manager for MS Windows\nchoco install wget\n  Gnu/Linux\nNote As Wget is a Gnu developed software, it’s available in most distributions main repositories, the installation process should be as simple as using your distribution’s package manager.   Examples\n   apt install wget\n  dnf install wget\n  pacman -S wget\n  macOS\n  Install using Homebrew package manager\nbrew install wget\n  Install using Macports\nport install wget\n   Pulling the website to your local machine Note We will be using some basic parameters for Wget which should work for the majority of websites, but you may need to refer to the manual pages of Wget in case of needing to do some tweaks or solve an issue with the resulting mirror.  wget --mirror --convert-links --adjust-extension --page-requisites http://example.org\nParameters and options description --mirror\nTurn on options suitable for mirroring. This option turns on recursion and time-stamping, sets infinite recursion depth and keeps FTP directory listings.\n--convert-links\nAfter the download is complete, convert the links in the document to make them suitable for local viewing. This affects not only the visible hyperlinks, but any part of the document that links to external content, such as embedded images, links to style sheets, hyperlinks to non-HTML content, etc.\n--adjust-extension\nIf some link points to //foo.com/bar.cgi?xyz with --adjust-extension asserted and its local destination is intended to be ./foo.com/bar.cgi?xyz.css, then the link would be converted to //foo.com/bar.cgi?xyz.css. Note that only the filename part has been modified. The rest of the URL has been left untouched, including the net path (\u0026quot;//\u0026quot;) which would otherwise be processed by Wget and converted to the effective scheme (ie. \u0026quot;http://\u0026quot;).\n--page-requisites\nThis option causes Wget to download all the files that are necessary to properly display a given HTML page. This includes such things as inlined images, sounds, and referenced stylesheets.\n Tip Wget2 is currently being developed, while it’s not stable yet but it’s a full rewrite of the original Wget and meant to replace it in the near future. Wget2 comes with many new features such as HTTP/2.0 support and multi-threaded download which can make the process of pulling large websites way faster.  Note For websites operating behind Cloudflare, this process can be identified as malicious behaviour as many simultaneous requests are coming from one IP address in short intervals, this can result in partial downloads or failing to download some assets such as inline images and CSS files.  This can be solved by either whitelisting your IP address on Cloudflare and disable assets protection features during the crawling process, or configure the origin server to allow direct access on a different domain/sub-domain with basic authentication enabled, you can then add --http-user=[HTTP-USER] --http-passwd=[HTTP-PASSWORD] parameters to your Wget command to authenticate.\n","excerpt":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | …","ref":"/docs/static-mirroring/wget-guide/","title":"Wget"},{"body":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | Android | BSD\n HTTrack is a free software developed for the specific purpose of downloading fully functional offline copies of any website.\n It has many advantages over Wget, and offers a graphical user interface. While it’s not being updated since 2017, it proofed to be efficient in most use cases in our testing scenarios.\n  Install HTTrack Windows\n  Chocolatey package manager for MS Windows\nchoco install httrack\n  Or you can download the installation file here\n  Gnu/Linux\nSince HTTrack is available in most major distributions main repositories, you can use your package manager to directly install the compiled version.   Example\n  apt install httrack dnf install httrack pacman -S httrack  In this page you can find instructions on downloading and building HTTrack from source.  Note Also, a version with a graphical user interface exists for Gnu/Linux but still in beta, you can find the source here  macOS\n  Using Homebrew package manager\nbrew install httrack\n  Using Macports\nsudo port install httrack\n   Pulling the website to your local machine httrack --mirror --robots=0 --stay-on-same-domain --user-agent \u0026#34;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:63.0) Gecko/20100101 Firefox/63.0\u0026#34; --keep-links=0 --path example.org --quiet https://example.org/ -* +example.org/* Parameters and options description --mirror\nMirror websites\n--robots=0\nFollow robots.txt and meta robots tags (0=never,1=sometimes,* 2=always, 3=always (even strict rules)) (\u0026ndash;robots[=N])\n--stay-on-same-domain\nStay on the same principal domain\n--user-agent \u0026quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:63.0) Gecko/20100101 Firefox/63.0\u0026quot;\nUser-agent field sent in HTTP headers\n--keep-links=0\nkeep original links (e.g. http://www.adr/link) (--keep-links=0 *relative link, --keep-links absolute links, --keep-links=4 original links, --keep-links=3 absolute URI links, --keep-links=5 transparent proxy link)\n--path example.org\nPath for mirror/logfiles+cache (--path mirror[,path cache and logfiles])\n--quiet\nno questions - quiet mode\nhttps://example.org/ -*\nReplace example.org with the website you want to mirror\n+example.org/*\nHTTrack will crawl and scan the whole website, renders every and save it locally to your machine in an offline browsable form. The suggested combination of arguments will convert the inline URLs to relative links which can be hosted virtually anywhere.\nhttrack --continue\nIn case of intercepted or uncompleted download process HTTrack will use the cache to resume the download process and make sure it won’t include re-downloading the same unchanged assets.\nhttrack --update\n  Here we need to consider a very important note about how HTTrack functions.\n Normally there will be a cached version of all downloaded assets saved in a directory under the main project’s directory named hts-cache, the cache will be used in every update presumably to avoid having to crawl and download the whole website with every update, which can be a very time-consuming process especially with big websites.\nHowever, in our testing with different websites of different sizes and structures, the real-life scenario turned to be different from what the software documentations provides.\nFor example with a big WordPress website running relatively recent version of Nginx web server and operating behind a Cloudflare proxy, the cache never helped to reduce the update times needed to refresh the static copy with new published content and HTTrack was almost crawling all over the website with every update.  This can be connected with many elements, among them will that HTTrack is a relatively old software which didn’t receive any updates since 2017 therefore it’s support for the most recent changes in the web structure and technologies isn’t the best.\nOne other element which will cause great impact and change of behavior in any tools doing the same functionality, will be dealing with different configuration and installations of Web servers, CMS, and security measurements.\nWe encourage you to dig deeper in HTTrack documentation to find options and arguments which can help to find the best suited configuration for your setup.\nNote When using HTTrack - and almost any website downloader -, while you are using Cloudflare proxy and DDoS protection for your website, it’s highly important to set a user-agent in the arguments and make sure the chosen user-agent isn’t blocked in the security settings in your web server and Cloudflare.  Note With big websites, many security settings and tools might identify the constant crawling and multiple hits in short time intervals as malicious behavior and block or throttle your IP address\u0026rsquo;s connection to the website.\nIn that case you should revise your security settings and find the maximum allowed connections then you can use arguments like --max-rate=N, --connection-per-second=N, --max-pause=N to limit HTTrack traffic hitting your website to the maximum allowed numbers.\nAlso, you should consider whitelisting your IP address in your security settings if the option exists.\n ","excerpt":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | …","ref":"/docs/static-mirroring/httrack-guide/","title":"HTTrack"},{"body":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | BSD\n WP2Static WP2Static options\n WP2Static can be considered as the most optimal plugin for the purpose of generating and updating a static mirror.\n This plugin also offers different deployment sub-plugins which can configure direct deployments to platforms like Amazon S3 and GitHub.\nHowever, it\u0026rsquo;s plugin development process itself doesn’t seem to be stable, at the time of writing these documentations, it appears to be transitioning to a new approach and undergoing big changes.\nAn old version of the plugin was recently flagged as closed in WordPress’s plugins platform, and the developed published his reasoning behind this approach.\nTwo different versions of the plugin however will continue to be available through the developer\u0026rsquo;s website and GitHub repository.\nThe old version renamed as static-html-output-plugin, will be available pre-built and ready to install on the website download section.\nThe new version named as WP2Static, while still being considered as experimental, it performed well in our testing but it needs to be built from the source before installing on WordPress.\n Downloading and building the new plugin   Clone the repository.\ngit clone https://github.com/WP2Static/wp2static.git\n  Install Composer and PHP on your local machine.\n  Change to the plugins source code directory.\n  Build the plugin using\ncomposer install\n  Compress the plugin directory into a .zip archive\nzip -r wp2static.zip wp2static\n  Note Same steps are required for each sub-plugin which you will need for automating deployments to differant hosting platforms such as Amazon S3 or GitHub.\n  Sub-plugins should be installed manually as separate plugins on WordPress.\n  Sub-plugins will show under Add-Ons section in WP2Static settings and needs to be enabled and configured.\n    WP2Static settings and configuration Select which content types should be included in the mirror, and the deployment URL\nAfter installing any deployment Add-On it should be displayed here\nExample for Amazon S3 deployment Add-On options\nHere you will find useful diagnostic information about your installation and it\u0026rsquo;s compatibility with the plugin requirements\nSimply static Simply Static options\nSimply Static is a WordPress Plugin with similar functionality to WP2Static. However, it comes with fewer features and might not fit the needs of a big and/or very frequently updated website, as for example, it doesn’t support incremental builds for the static mirror, which means it needs to go through the whole process every time a build is triggered.\nFurthermore, Simply Static doesn’t support automatic builds triggering with every new post, so a new build can only be triggered manually, which makes this plugin more fit for small blogs or rarely updated websites.\nSimply Static settings and configuration Simply Static options\nNote   You can set the Delivery Method to Local Directory to have the generated static files stored on the same server/machine\n  Make sure the web-server user have write privileges on the selected directory\n  If you are planning on hosting your static website on an object storage platform such as S3, you should make sure to Use relative URLs\n   Here you will find useful diagnostic information about your installation and it\u0026rsquo;s compatibility with the plugin requirements\n","excerpt":"Required expertise level : Beginner / Intermediate\nPlatform : Gnu/Linux | macOS | MS Windows | BSD …","ref":"/docs/static-mirroring/wpplugins/","title":"WordPress Plugins"},{"body":"What is dynamic-mirroring  There are multiple ways of mirroring a dynamic website.\n One of these ways will be creating a reverse-proxy server which acts as a middle box between the client and the origin server which hosts the website.\nThe server in the middle will get the required URL from the client and contact the origin server to fetch the content and serve it to the user.\nIn this method the reverse-proxy server will be reachable through a domain-name which isn’t blocked, this method is relatively easy to implement, but it can be easily blocked by blocking the domain name or the IP address of the proxy server.\nAnother method will be hosting a full copy of the original blocked website on the server and keeping it synchronized with the original server.\nThis method needs certain access privileges to the origin server and it holds the same disadvantages of the first method. In many cases you can just keep changing the mirror server’s domain name and IP address whenever it gets blocked, but that can be inefficient in terms of both financial costs and usability.\nIn our documentations, we will try to introduce different methods of dynamic mirroring, either by creating a proxy service hosted on Google App Engine using Decoy, or creating Onion service mirror on Tor network using EOTK - The Enterprise Onion Toolkit.\n","excerpt":"What is dynamic-mirroring  There are multiple ways of mirroring a dynamic website.\n One of these …","ref":"/docs/dynamic-mirroring/","title":"Dynamic Mirroring"},{"body":" In this section, we will go through some of the cloud service providers which offers the functionality of hosting and serving a static website.\n ","excerpt":" In this section, we will go through some of the cloud service providers which offers the …","ref":"/docs/static-mirroring/static-hosting/","title":"Static mirror hosting"},{"body":"","excerpt":"","ref":"/docs/alternative-publishing-methods/","title":"Alternative Publishing Methods"},{"body":"Required expertise level : Intermediate\nPlatform : Any\n  Amazon S3 (Amazon Simple Storage Service) is a data storage service and one of the Amazon Web Services (AWS)\n  S3 provides object storage service, which means that data is stored and addressed as objects, each object contains it\u0026rsquo;s own data in addition to meta-data and a unique identifier.\nObject storage is often used to store big amounts of data that doesn\u0026rsquo;t need the features and structure of the file systems hierarchy.\nThe main advantage of using S3 to host our static mirror is the ability to serve a fully functional \u0026ldquo;static\u0026rdquo; web pages directly from Amazon S3 URLs which consists of [bucketname]+[endpoint] or [endpoint]/[bucketname] ex: mybucket.s3.us-east-2.amazonaws.com or https://s3.us-east-2.amazonaws.com/mybucket\n Configure static site hosting on S3 Creating S3 bucket, and setting configurations for Static Site hosting  First, you need to create an account on Amazon Web Services (AWS), and then, sign in to AWS Console.  AWS login page\n In S3 management console, create a new bucket.  S3 management console\nNote You should pick a unique name for your bucket.  S3 new bucket\n In Set permissions tab, uncheck Block all public access, and confirm that you want to enable public access to the bucket.  Bucket permission tab\n Finally, review your options and create the new bucket.  Create bucket\n Your newly created bucket should appear in this form.   Head to the new bucket\u0026rsquo;s settings by clicking on the bucket name.  S3 bucket settings\n In Properties tab click on Static website hosting  S3 bucket Properties\n After setting Use this bucket to host a website, you need to set the path for your index.html(main page), optionally you can set a custom error.html page, and click save.  S3 Static website hosting\nS3 enabled Static website hosting\nNow, we need to set the bucket permissions policy to allow public read for all objects.   Head to Permissions tab, and click on Bucket Policy.  S3 bucket policy\n The simplest form of static website hosting policy on S3 should look like this.  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::example.com/*\u0026#34; ] } ] }  Note In this example bucket policy example.com is the bucket name. To use this policy example you need to replace example.com with your newly created bucket name in the \u0026quot;Resources\u0026quot; key value.  Now, we are ready to upload our static mirror to our S3 bucket.\n Upload using the Web user interface  You will find the upload option in the bucket settings page.  S3 files upload\n Make sure grant public read access to the uploaded files in the permissions tab.  S3 files upload permissions\nNote While you can easily upload your website files directly in the browser by clicking on Upload in the bucket settings page, it\u0026rsquo;s preferred to use AWS CLI, specially if when you are uploading large websites.   Install AWS Command Line Interface   MS Windows\n Download and install the official installation file Install using Chocolatey windows package manager choco install awscli    Gnu/Linux\n Normally, you will fine AWS CLI package available in your distribution software repositories, in that case you can simply use your package manager to install it directly. ex: apt install awscli If that\u0026rsquo;s not the case, you can install it manually by executing these commands in your terminal in their respective order.    Linux x86 (64-bit)\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Linux ARM\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install  macOS  Install using Homebrew package manager. brew install awscli     Upload using AWS CLI  First, you should configure awscli and grant it access to your AWS account, for that you will need to get your AWS Access Key ID and Secret Access Key, you can create new Access Keys by going to AWS IAM (Identity and Access Management) Dashboard.  Alert Make sure to store the generated keys securely and don\u0026rsquo;t share them over unsecured medium, the keys can be used to gain access to your AWS account data.  awscli configure   ِMove to the website local directory.\n  Upload your files by executing replace [bucket-name] with the name of the bucket you created on S3\n  aws s3 sync . s3://[bucket-name]/  Now you can test your new static mirror, you\u0026rsquo;ll find the website URL in bucket settings \u0026gt; Properties \u0026gt; Static website hosting\n Find the configured URL for your website here\nNote Note that the provided URL here is http://mystaticwebsitetest.s3-website.us-east-2.amazonaws.com can only be accessed on plain-text HTTP protocol  Now, there are two different URL structures which allows accessing your static website on the secure protocol HTTPS\n Bucket name as a sub-domain https://[mystaticwebsitetest].s3.us-east-2.amazonaws.com   Bucket name in the path https://s3.us-east-2.amazonaws.com/[mystaticwebsitetest]  Note While both methods may achieve same results, it\u0026rsquo;s preferred in censorship circumvention context to include the bucket name in the path as in method number 2.\nAWS uses a Wildcard SSL certificate which supports any sub-domain under *.s3.us-east-2.amazonaws.com, but defining the bucket name in the URL path would make it more difficult to detect traffic to this particular bucket/region/endpoint through Deep Packet Inspection(DPI)\n ","excerpt":"Required expertise level : Intermediate\nPlatform : Any\n  Amazon S3 (Amazon Simple Storage Service) …","ref":"/docs/static-mirroring/static-hosting/amazons3/","title":"Hosting on Amazon S3"},{"body":"Required expertise level : Intermediate\nPlatform : Any\n  Google Cloud Storage is object storage service similar to Amazon S3, and it provides the ability to serve static web pages as well.\n  The main advantage of using Google Cloud Storage to host our static mirror is the ability to serve a fully functional \u0026ldquo;static\u0026rdquo; web pages directly using Google Cloud Platform URLs which usually looks like this: https://storage.googleapis.com/[bucketname]\n Configure static site hosting on Google Cloud Storage Creating GCS bucket, and setting permissions  First, you need to create an account on Google Cloud Platform, you will need to create a Google account for this.  Note While it\u0026rsquo;s possible to use a pre-existing Google account for this step, it\u0026rsquo;s better to create a new one just for this purpose.   Log in to Google Cloud Console using your newly created account.   Here, you\u0026rsquo;ll need to open Storage from the side bar to access Google Cloud Storage settings page.  Create a new bucket here\nBuck name\n First step will be choosing your bucket name, bucket names should be unique and you\u0026rsquo;ll use it to access your static website later.  Region settings\n Unless you know what you are doing, there is no need to change anything here.  Storage Class\n In most cases it should be Standard.  Access control\n We will be handling permissions and access control configurations later on, so we\u0026rsquo;ll leave it unchanged for now.  Encryption\n As this bucket will be publicly available, we don\u0026rsquo;t need to change anything with Encryption settings.  Now, we can go proceed to the new bucket settings page.   Upload the static website While you can upload your static website using the web interface, it\u0026rsquo;s preferred to do that using Google Cloud SDK, specially if you are planning on updating your mirror with new content frequently.  Install Google Cloud SDK Command Line Interface Installing Google Cloud SDK is pretty straight forward process. Simply, follow the instructions related to your operating system in this guide and you will be good to go.\nUpload using Google Cloud SDK CLI  After installing Google Cloud Sdk, you will need to authorize the local client to connect to you Google Cloud account, you can do that by opening your terminal and entering gcloud init   After Successfully logging in to your account, you will be asked to select the project you want to use, if you didn\u0026rsquo;t create a new project you can do that now using the local CLI.  Now, you can use your local CLI to upload the static mirror to the newly created bucket.   Using your terminal, change directory to the static mirror files location, and enter gsutil rsync -R [local-dir] gs://[bucketname]  Note Replace [local-dir] with the static mirror directory name and [bucketname] with the bucket name.  Uploading files\n Now if you go to your bucket settings page, you will notice the static mirror files are uploaded.   Final step will be setting public permissions to the files so the static mirror will be accessible to public internet, you can do that using the local CLI by entering gsutil iam ch allUsers:objectViewer gs://[bucketname]  Note Replace [bucketname] with your bucket name.  Now you can access your static mirror using this URL scheme https://storage.googleapis.com/[bucketname]/index.html\nNote Replace [bucketname] with your bucket name.  Updating the static mirror Updating your static mirror with new content will be as simple as going to the mirror\u0026rsquo;s local directory on your terminal and executing gsutil rsync -R [local-dir] gs://[bucketname] every time.  ","excerpt":"Required expertise level : Intermediate\nPlatform : Any\n  Google Cloud Storage is object storage …","ref":"/docs/static-mirroring/static-hosting/gcs/","title":"Hosting on Google Cloud Storage"},{"body":" This project is hoping to create a knowledge repository of accumulated experience in the fight for a free and open Internet. All our documentations, guides, code, and research are published under GNU General Public License (GPLv3)\nWe welcome and appreciate any contributions, suggestions, and critique.\n We use Hugo to format and generate our website, the Docsy theme for styling and site structure, and GitHub to deploy and host the site.\nHugo is an open-source static site generator that provides us with templates, content management in a standard directory structure, and a website generation engine. You write the pages in Markdown (or HTML if you want), and Hugo wraps them up into a website.\nThis Project and all it\u0026rsquo;s related materials are hosted on GitHub.\nWe use GitHub pull requests to receive contributions and submissions, Consult GitHub Help for more information on using pull requests.\n Creating an issue If you\u0026rsquo;ve found a problem in the docs, but you\u0026rsquo;re not sure how to fix it yourself, please create an issue in the Documentations repo. You can also create an issue about a specific page by clicking the Create Issue button in the top right hand corner of the page.\nUseful resources  Docsy user guide: All about Docsy Hugo theme, including how it manages navigation, look and feel, and multi-language support. Hugo documentation: Comprehensive reference for Hugo. Github Hello World!: A basic introduction to GitHub concepts and workflow.  Updating a single page If you\u0026rsquo;ve just spotted something you\u0026rsquo;d like to change while using the docs, Docsy has a shortcut for you:\n1- Click Edit this page in the top right hand corner of the page. 2- If you don\u0026rsquo;t already have an up to date fork of the project repo, you are prompted to get one - click Fork this repository and propose changes or Update your Fork to get an up to date version of the project to edit. The appropriate page in your fork is displayed in edit mode.\nPreviewing your changes locally If you want to run your own local Hugo server to preview your changes as you work:\n1- Follow the instructions here Getting started to install Hugo and any other tools you need. You\u0026rsquo;ll need at least Hugo version 0.45 (we recommend using the most recent available version), and it must be the extended version, which supports SCSS.\n2- Fork the project website repository repo into your own project, clone it to your local machine\ngit clone [your fork address] 3- Run hugo server in the site root directory.\n By default your site will be available at http://localhost:1313/. Now that you\u0026rsquo;re serving your site locally, Hugo will watch for changes to the content and automatically refresh your site.\n 4- Continue with the usual GitHub workflow to edit files, commit them, push the changes up to your fork, and create a pull request.\nLocalization We hope our documentations will be accessible to as many people as possible, if you find it useful and willing to localize it to unavailable language, or have suggestions to improve an existing translation, we welcome and appreciate your efforts, we use GitHub workflow.\nAfter forking the website repository to your own project/account\n Create a directory under content i.e. content/ar Add the new language under [languages] in config.toml  Example of the required values [languages.en] title = \u0026#34;Icarus Project\u0026#34; description = \u0026#34;Testign and documenting Internet censorship circumvention techniques\u0026#34; languageName =\u0026#34;English\u0026#34; languagedirection = \u0026#34;ltr\u0026#34; contentDir = \u0026#34;content/en\u0026#34; weight = 1 time_format_default = \u0026#34;02.01.2006\u0026#34; time_format_blog = \u0026#34;02.01.2006\u0026#34;  Follow Docsy\u0026rsquo;s guide on how to translate the theme\u0026rsquo;s UI strings (text for buttons etc.) Submit your changes in a pull request  available Languages   Arabic (coming soon!)\n  English\n  ","excerpt":"This project is hoping to create a knowledge repository of accumulated experience in the fight for a …","ref":"/docs/contribution-guidelines/","title":"Contribution Guidelines"},{"body":"Required expertise level : Intermediate\nPlatform : Any\n  GitHub is a software development platform using Git version control system, and it provides the ability to serve static web pages through GitHub Pages service.\n  Note GitHub provides an easy step-by-step guide on how to host a static website on their platform. The guide is available in English, so, in order to avoid redundancy we\u0026rsquo;ll just refer to it here and add a translated version for every available language in our documentations.   Install Git   MS Windows\n Download and install the official installation file Install using Chocolatey windows package manager choco install git    Gnu/Linux\n Normally, you will fine Git package available in your distribution software repositories, in that case you can simply use your package manager to install it directly. ex: apt install git    macOS\n Install using Homebrew package manager. brew install git    ","excerpt":"Required expertise level : Intermediate\nPlatform : Any\n  GitHub is a software development platform …","ref":"/docs/static-mirroring/static-hosting/github/","title":"Hosting on GitHub"},{"body":" Open Observatory of Network Interference (OONI) CloudFlare : Encrypt that SNI: Firefox edition CloudFlare : Encrypt it or lose it: how encrypted SNI works IETF : Transport Layer Security (TLS) Extensions IETF : TLS Encrypted Client Hello - draft-ietf-tls-esni-07 Chromium Support Status : Issue 908132 Developing ESNI for OpenSSL (DEfO) Mozilla : Encrypted SNI Comes to Firefox Nightly - October 18, 2018 If you want to test ESNI further, or develop something for it - Noctilucent For better understanding of DNS over HTTPS (DoH) and other DNS privacy related materials IPFS - Get Started The Enterprise Onion Toolkit (EOTK) Tor Project website If Internet censorship is in place in your country, there is a fairly chance this website is blocked Tor Project repository on GitHub The Internet Archive Google Drive EFF Tor mirror Calyx Institute Tor mirror Chaos Computer Club Tor mirror We host an expermintal mirror for Tor Browser binary files on IPFS, you can reach it on ipfs.io/ipns/tor-ipfs.fightcensorship.tech Qurium - Bifrost project NetBlock reports  ","excerpt":"Open Observatory of Network Interference (OONI) CloudFlare : Encrypt that SNI: Firefox edition …","ref":"/docs/reference/","title":"References"},{"body":" In this section you\u0026rsquo;ll find the project\u0026rsquo;s documentations, which will be updated regularly\n Note Icarus project related documentations are currently available in these Languages.   Arabic (coming soon!) English  Note We are planning to localize the project documentations and guides to as many languages as possible. If you find it useful and want to translate it to any language, see the Contribution Guidelines section!  ","excerpt":" In this section you\u0026rsquo;ll find the project\u0026rsquo;s documentations, which will be updated …","ref":"/docs/","title":"Documentation"},{"body":"   With the first release of Icarus Project\u0026rsquo;s documentations, we are happy to announce the results of our research and documentation process.  The project idea started as an individual initiative in 2017, trying to collect, test, document, and develop Internet censorship circumvention techniques as a response to the rise of Internet censorship at the time.\nIn 2019 the idea started to be realized through one year fellowship program supported by Open Tech Fund (OTF) Information Controls Fellowship program, and hosted by Stratosphere Lab at the Czech Technical University in Prague (CVUT).\nWe started by collecting, testing, and documenting as many ideas and methods as we can.\nNow, we present these guides and documentations to serve as the core of this project. We are hoping that this initial release should it prove - useful and effective - will receive and integrate different contributions from the interested communities, towards developing and maintaining an accessible and user-friendly online repository of accumulative experience and research.\nFeel free to reach out and let us know what do you think and how can we improve, checkout the community page, also, checkout the Contribution Guidelines if you are interested!  ","excerpt":"With the first release of Icarus Project\u0026rsquo;s documentations, we are happy to announce the …","ref":"/blog/2020/09/19/introducing-icarus-project/","title":"Introducing Icarus Project"},{"body":" About Icarus Project        Icarus Project is a technical research laporatory dedicated for testing, analyzing, documenting, and developing Internet censorship circumvention solutions. We are aiming on collceting as many techniques as possible and document then in forms of technical analysis, and step-by-step implementation guides.     The Project is mainly focused on publisher-side circumvention methods/techniques, but we will also include some client side implementations      This project is the result of Open Tech Fund (OTF) - Information Controls Fellowship - program, and hosted by Stratosphere Lab at the Czech Technical University in Prague (CVUT).. Learn more about Open Tech Fund (OTF) and the threat to Internet freedom.\n    ","excerpt":"About Icarus Project        Icarus Project is a technical research laporatory dedicated for testing, …","ref":"/about/","title":"About Icarus Project"},{"body":"","excerpt":"","ref":"/blog/","title":"Icarus Project Blog"},{"body":"","excerpt":"","ref":"/community/","title":"Community"},{"body":" Welcome to Icarus Project Learn More   Code   An online repository for documented Internet censorship circumvention techniques and methods.\n\n        Implementation guides and technical evaluations  We provide step-by-step implementation guides and technical evaluations to reduce the technical and research efforts needed on the publisher’s side in order to have their own mirrors.       We test and document Internet censorship circumvention techniques       Get in contact! If you have any questions or ideas.\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New contributors are always welcome!\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"Welcome to Icarus Project Learn More   Code   An online repository for documented Internet …","ref":"/","title":"Home"},{"body":"","excerpt":"","ref":"/mirrors/","title":"Mirrors"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"}]